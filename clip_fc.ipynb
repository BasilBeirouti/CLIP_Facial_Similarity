{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f10667d9-12dd-474f-a3a7-00f061cdbc35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ftfy\n",
      "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex\n",
      "  Downloading regex-2023.10.3-cp311-cp311-macosx_11_0_arm64.whl (291 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.0/291.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /Users/bbeirouti/.pyenv/versions/3.11.3/envs/torch/lib/python3.11/site-packages (from ftfy) (0.2.12)\n",
      "Installing collected packages: tqdm, regex, ftfy\n",
      "Successfully installed ftfy-6.1.3 regex-2023.10.3 tqdm-4.66.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /private/var/folders/t5/39xr8mt505b9qkc4qncmwp7r0000gn/T/pip-req-build-n8784vv7\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /private/var/folders/t5/39xr8mt505b9qkc4qncmwp7r0000gn/T/pip-req-build-n8784vv7\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /Users/bbeirouti/.pyenv/versions/3.11.3/envs/torch/lib/python3.11/site-packages (from clip==1.0) (6.1.3)\n",
      "Requirement already satisfied: regex in /Users/bbeirouti/.pyenv/versions/3.11.3/envs/torch/lib/python3.11/site-packages (from clip==1.0) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /Users/bbeirouti/.pyenv/versions/3.11.3/envs/torch/lib/python3.11/site-packages (from clip==1.0) (4.66.1)\n",
      "Requirement already satisfied: torch in /Users/bbeirouti/.pyenv/versions/3.11.3/envs/torch/lib/python3.11/site-packages (from clip==1.0) (2.2.0.dev20231206)\n",
      "Requirement already satisfied: torchvision in /Users/bbeirouti/.pyenv/versions/3.11.3/envs/torch/lib/python3.11/site-packages (from clip==1.0) (0.17.0.dev20231206)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /Users/bbeirouti/.pyenv/versions/3.11.3/envs/torch/lib/python3.11/site-packages (from ftfy->clip==1.0) (0.2.12)\n",
      "Requirement already satisfied: filelock in /Users/bbeirouti/.pyenv/versions/3.11.3/envs/torch/lib/python3.11/site-packages (from torch->clip==1.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/bbeirouti/.pyenv/versions/3.11.3/envs/torch/lib/python3.11/site-packages (from torch->clip==1.0) (4.9.0rc1)\n",
      "Requirement already satisfied: sympy in /Users/bbeirouti/.pyenv/versions/3.11.3/envs/torch/lib/python3.11/site-packages (from torch->clip==1.0) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/bbeirouti/.pyenv/versions/3.11.3/envs/torch/lib/python3.11/site-packages (from torch->clip==1.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/bbeirouti/.pyenv/versions/3.11.3/envs/torch/lib/python3.11/site-packages (from torch->clip==1.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/bbeirouti/.pyenv/versions/3.11.3/envs/torch/lib/python3.11/site-packages (from torch->clip==1.0) (2023.12.1)\n",
      "Requirement already satisfied: numpy in /Users/bbeirouti/.pyenv/versions/3.11.3/envs/torch/lib/python3.11/site-packages (from torchvision->clip==1.0) (1.26.2)\n",
      "Requirement already satisfied: requests in /Users/bbeirouti/.pyenv/versions/3.11.3/envs/torch/lib/python3.11/site-packages (from torchvision->clip==1.0) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/bbeirouti/.pyenv/versions/3.11.3/envs/torch/lib/python3.11/site-packages (from torchvision->clip==1.0) (10.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/bbeirouti/.pyenv/versions/3.11.3/envs/torch/lib/python3.11/site-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/bbeirouti/.pyenv/versions/3.11.3/envs/torch/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/bbeirouti/.pyenv/versions/3.11.3/envs/torch/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/bbeirouti/.pyenv/versions/3.11.3/envs/torch/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/bbeirouti/.pyenv/versions/3.11.3/envs/torch/lib/python3.11/site-packages (from requests->torchvision->clip==1.0) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/bbeirouti/.pyenv/versions/3.11.3/envs/torch/lib/python3.11/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
      "Installing collected packages: clip\n",
      "\u001b[33m  DEPRECATION: clip is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running setup.py install for clip ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed clip-1.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ftfy regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82a1ef1-b983-4e4a-93fb-1bf835dc125e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.datasets import fetch_lfw_pairs\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Preprocess and Encode Images\n",
    "def compute_clip_features(dataset, clip_model, preprocess, device):\n",
    "    clip_features = []\n",
    "    for image_pair in dataset:\n",
    "        image1, image2 = image_pair[0], image_pair[1]\n",
    "\n",
    "        # Preprocess images\n",
    "        image1 = Image.fromarray(image1.astype('uint8'), 'RGB')\n",
    "        image2 = Image.fromarray(image2.astype('uint8'), 'RGB')\n",
    "        preprocessed_image1 = preprocess(image1).unsqueeze(0).to(device)\n",
    "        preprocessed_image2 = preprocess(image2).unsqueeze(0).to(device)\n",
    "\n",
    "        # Encode images\n",
    "        with torch.no_grad():\n",
    "            features1 = clip_model.encode_image(preprocessed_image1)\n",
    "            features2 = clip_model.encode_image(preprocessed_image2)\n",
    "\n",
    "        # Flatten and concatenate features for each image pair\n",
    "        features1 = features1.view(-1)\n",
    "        features2 = features2.view(-1)\n",
    "        concatenated_features = torch.cat((features1, features2))\n",
    "\n",
    "        clip_features.append(concatenated_features.cpu())\n",
    "\n",
    "    return clip_features\n",
    "\n",
    "\n",
    "class LFWDatasetFeatures(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.features[idx]\n",
    "        label = self.labels[idx]\n",
    "        return features, label\n",
    "\n",
    "class CustomClassifierConcat(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomClassifierConcat, self).__init__()\n",
    "\n",
    "        # Assuming each CLIP feature vector is of size 512, concatenated size is 1024\n",
    "        self.fc1 = nn.Linear(1024, 2048)  # Input size for concatenated features\n",
    "        self.bn1 = nn.BatchNorm1d(2048)\n",
    "        self.fc2 = nn.Linear(2048, 1024)\n",
    "        self.bn2 = nn.BatchNorm1d(1024)\n",
    "        self.fc3 = nn.Linear(1024, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, features):\n",
    "        out = self.fc1(features)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "# Training and evaluation functions\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for features, labels in train_loader:\n",
    "        features = features.to(device).float()  # Ensure features are float32\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.sigmoid(outputs).round()\n",
    "        correct += (preds.squeeze() == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    average_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct / total_samples\n",
    "    return average_loss, accuracy\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            features, labels = features.to(device).float(), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs.squeeze(), labels.float())\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.sigmoid(outputs).round()\n",
    "            correct += (preds.squeeze() == labels).sum().item()\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    return total_loss / len(test_loader), accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "902ab7fe-33d9-40a7-a8a5-bd1aa0fcbf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_small_dataset(features, labels, subset_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Create a smaller subset of the dataset for quick prototyping.\n",
    "    :param features: The original features.\n",
    "    :param labels: The corresponding labels.\n",
    "    :param subset_ratio: The fraction of the dataset to use.\n",
    "    :return: A tuple of (small_features, small_labels).\n",
    "    \"\"\"\n",
    "    small_features, _, small_labels, _ = train_test_split(\n",
    "        features, labels, test_size=subset_ratio, random_state=42\n",
    "    )\n",
    "    return small_features, small_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfd5c8e7-eb37-4224-8926-0f4596918196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm test_features.pt train_features.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cbe64bf-3065-4ed4-866f-22e33d33d4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded features from disk.\n"
     ]
    }
   ],
   "source": [
    "# Main execution starts here\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "lfw_pairs_train, lfw_pairs_test = fetch_lfw_pairs(subset='train', color=True), fetch_lfw_pairs(subset='test', color=True)\n",
    "X_train_full, y_train_full, X_test_full, y_test_full = lfw_pairs_train.pairs, lfw_pairs_train.target, lfw_pairs_test.pairs, lfw_pairs_test.target\n",
    "small_train, small_train_y = create_small_dataset(X_train_full, y_train_full)\n",
    "small_test, small_test_y = create_small_dataset(X_test_full, y_test_full)\n",
    "\n",
    "# Optionally create a smaller dataset for quick prototyping\n",
    "use_small_dataset = False  # Set to False to use the full dataset\n",
    "\n",
    "if use_small_dataset:\n",
    "    X_train_full, y_train_full, X_test_full, y_test_full = small_train, small_train_y, small_test, small_test_y\n",
    "    \n",
    "# Check if precomputed features are already saved\n",
    "def load_features(filename):\n",
    "    if os.path.exists(filename):\n",
    "        return torch.load(filename)\n",
    "    return None\n",
    "\n",
    "train_features = load_features('train_features.pt')\n",
    "test_features = load_features('test_features.pt')\n",
    "\n",
    "if train_features is None or test_features is None:\n",
    "    print(\"Computing features...\")\n",
    "    train_features = compute_clip_features(X_train_full, model, preprocess, device)\n",
    "    test_features = compute_clip_features(X_test_full, model, preprocess, device)\n",
    "    torch.save(train_features, 'train_features.pt')\n",
    "    torch.save(test_features, 'test_features.pt')\n",
    "else:\n",
    "    print(\"Loaded features from disk.\")\n",
    "\n",
    "use_small_dataset = False\n",
    "\n",
    "if use_small_dataset:\n",
    "    small_train_features, small_train_labels = create_small_dataset(train_features, y_train_full)\n",
    "    small_test_features, small_test_labels = create_small_dataset(test_features, y_test_full)\n",
    "\n",
    "    train_dataset = LFWDatasetFeatures(small_train_features, small_train_labels)\n",
    "    test_dataset = LFWDatasetFeatures(small_test_features, small_test_labels)\n",
    "else:\n",
    "    train_dataset = LFWDatasetFeatures(train_features, y_train_full)\n",
    "    test_dataset = LFWDatasetFeatures(test_features, y_test_full)\n",
    "\n",
    "# Proceed with DataLoader, Model, Training, and Evaluation\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1851e6-afaf-419a-b369-aaecb8dfc794",
   "metadata": {},
   "source": [
    "We train a fully connected linear layer on top of CLIP on the LFW dataset from scikit-learn. \n",
    "The dataset consists of pairs of images of faces. Each image is 67 x 47 x 3\n",
    "The labels are True/False indicating whether the faces are of the same individual or not.\n",
    "Thus we train a simple binary classifcation fully connected linear layer on top of CLIP. CLIP weights are frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e57d461-3e6c-41ab-9b96-5b1bebd29126",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\"\n",
    "\n",
    "custom_model_concat = CustomClassifierConcat().to(device).float()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(custom_model_concat.parameters(), lr=0.0005)\n",
    "\n",
    "# Main training loop\n",
    "num_epochs = 1000\n",
    "i = 0\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train(custom_model_concat, train_loader, criterion, optimizer, device)\n",
    "    test_loss, test_accuracy = evaluate(custom_model_concat, test_loader, criterion, device)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"\"\"Epoch {epoch}: \n",
    "        Train Loss: {train_loss:.4f},\n",
    "        Train Accuracy: {train_accuracy:.2f}, \n",
    "        Test Loss: {test_loss:.4f}, \n",
    "        Test Accuracy: {test_accuracy:.2f}\"\"\")\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d67a69-a543-418c-b452-b66041d062b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
